---
title: "Assignment 1"
output: pdf_document
date: "2024-10-28"
editor_options: 
  markdown: 
    wrap: 72
---

----------------------------------TASK
1----------------------------------------
------------------------------------(1)-----------------------------------------
After importing the data, and appropriately initializing the different
adjacency matrices we build a simple QAP model for testing the
relationship between the advice network (Id = 1) and the friendship
network (Id = 2). Based on what we're asked to add on the subsequent
parts of task 1, we're supposed to take the friendship network as the
response network, and the advice network as the predictor network. After
regression, we get the following results

|           | theta      | exp(theta) | p-value |
|-----------|------------|------------|---------|
| Intercept | -1.4987723 | 0.2234043  | 0.0160  |
| advice    | 0.7255825  | 2.0659341  | 0.0284  |

The positive sign of the coefficient tied to the predictor network
suggests that naming someone as friend increases the probability of
asking them for advice. This suggestion can be seen to be of note by
taking a look at the computed p-values. Both fall within the typical
alpha level of 0.05.
---------------------------------------(2)--------------------------------------
(i) Hypothesis we have to test: "Friendship nomination is more likely
between pairs of managers within the same department." Clearly the
adjacency matrix that we have to go with is adj_same_department which is
a 21x21 matrix whose entry in row i and column j for i,j in {1,...,21}
is 1 whenever node i and node j are from the same department, and 0
otherwise.

(ii) Hypothesis we have to test: "Senior managers (\<-\> nodes with high
     "nodeTenure") are less likely to nominate friends" The idea is to
     have a matrix that for the entry corresponding to arc (i,j) stores
     the age of node i, since we need to test how senior managers
     nominate friends (not the other way around)

(iii) Hypothesis we have to test: "A friendship nomination is more
      likely between a pair of managers of a similar age." The idea here
      is simple enough, we just initialize a matrix that stores the
      information between differences in age. However the entries have
      to be "low" whenever the ages of the nodes is not similar, and
      high otherwise, so we use the following:

wght_similar_age[i,j] \<- 1/(abs(all_nodes[i, "nodeAge"] - all_nodes[j,
"nodeAge"])+1)

So we have wght_similar_age[i,j] = 1 whenever i and j have the same age,
and a lower value as the age difference between i and j increases.
--------------------------------------(3)---------------------------------------
After performing MR-QAP for the model specified in (2) we get the
following results:

|             | theta      | exp(theta)   | p-value |
|-------------|------------|--------------|---------|
| Intercept   | -1.4987723 | 1.783013e-11 | 0.0000  |
| advice      | 0.3655603  | 1.441321e+00 | 0.8942  |
| same dep.   | 0.9038062  | 2.468983e+00 | 0.3714  |
| seniority   | 42.1729747 | 2.067713e+18 | 0.9980  |
| similar age | 2.2283941  | 9.284943e+00 | 0.3280  |

All of those, except for the intercept hold very little statistical
significance, as they do not fit within the alpha level of 0.05.
Apparently there is too much noise for MR-QAP to find any meaningful
relationship between some of the predictor networks and the response
network.
------------------------------------(4)-----------------------------------------
Another hypothesis we could test for is the following: "Everyone asks
for advice to someone in the company that is in a higher position". We
would need to make a weight matrix for the difference in level, and then
use QAP as in the other cases.
------------------------------------(5)-----------------------------------------
The weight matrix described in (4) could be taken to be:

wght_level[i,j] \<- all_nodes[j, "nodeLevel"] - all_nodes[i,
"nodeLevel"]

so wght_leve[i,j] is positive whenever j is at a higher level, and it is
negative if the opposite holds. However, as there's no reason in having
a higher weight if the difference is higher, it's better to take the
maximum of that difference with the number one. So we take:

wght_level[i,j] \<- max(all_nodes[j, "nodeLevel"] - all_nodes[i,
"nodeLevel"],1)

|             | theta       | exp(theta)   | p-value |
|-------------|-------------|--------------|---------|
| Intercept   | -22.9972117 | 1.029053e-10 | 0.0212  |
| advice      | 0.3473968   | 1.415378e+00 | 0.8924  |
| same dep.   | 0.8489238   | 2.337130e+00 | 0.3906  |
| seniority   | 42.1422672  | 2.005183e+18 | 0.9978  |
| similar age | 1.0851201   | 2.959795e+00 | 0.3506  |
| lvl. diff.  | -1.6584837  | 1.904275e-01 | 0.7994  |

Again, the high p values don't really allow us to see anything but
noise. Even though the p-value for the intercept is relevant, the fact
that it significantly changed from the previous model could be an
indication that the level difference matrix added more noise than
anything.

----------------------------------TASK
2----------------------------------------

```{r}
# Network Modeling - HS 2024
# C. Stadtfeld, A. Uzaheta and I. Smokovic
# Social Networks Lab
# Department of Humanities, Social and Political Sciences
# ETH Zurich
# 14 October 2024
#
# Assignment 1 - Task 2


# MHstep ------------------------------------------------------------------
```

Simulate the next step of a network in Markov chain using
Metropolis-Hasting

The function `MHstep` simulates the the Metropolis-Hastings step that
defines the Markov chain whose stationary distribution is the ERGM with
edge, mutual and nodematch statistics

@param net an object of class `matrix`. Adjacency matrix of the network.
@param theta1 a numeric value. The value of the edge parameter of the
ERGM. @param theta2 a numeric value. The value of the mutual parameter
of the ERGM. @param theta3 a numeric value. The value of the istar(2)
parameter of the ERGM.

@return next state of the Markov Chain

@examples MHstep( matrix(c(0, 1, 0, 0, 0, 0, 1, 1, 0), nrow = 3, ncol =
3), -log(0.5), log(0.4), log(.8) )

```{r}
MHstep <- function(net, theta1, theta2, theta3){
  
  # Number of vertices in the network
  nvertices <- nrow(net) 
  
  # Choose randomly two vertices, prevent loops {i,i} with replace = FALSE
  tie <- sample(1:nvertices, 2, replace = FALSE) 
  i <- tie[1]
  j <- tie[2]
  
  # Compute the change statistics
  #                --- MISSING---
  # we toggle i->j: 
  delta_edges <- ifelse(net[i,j] == 1, -1, 1)  # if there is an edge from i->j, we remove it otherwise we add one
  delta_mutual <- ifelse(net[i,j] == 0, net[j,i], -net[j,i]) # mathematically, we have delta_mutual = x_ji*(x_new_ij - x_ij) 
  delta_in2stars <- 0  # the indegree x_{+i} for node i is not affected, since we only toggle i->j
  
  # Compute the probability of the next state 
  # according to the Metropolis-Hastings algorithm
  #                --- MISSING---
  p = min(1, exp(theta1*delta_edges + theta2*delta_mutual + theta3*delta_in2stars)) # this is the formula on slide 19
  
  # Select the next state: 
  #                --- MISSING---
  # This choice is somewhat arbitrary: We toggle i->j, if the computed probability p is larger than a given sample of U(0,1)
  if (runif(1) <= p) {
    # Toggle the tie
    net[i, j] <- 1 - net[i,j]
  }
  
  # Return the next state of the chain
  return(net)
}
# sanity check
# MHstep(matrix(c(0, 1, 0, 0, 0, 0, 1, 1, 0), nrow = 3, ncol = 3), -log(0.5), log(0.4), log(.8))

# Markov Chain simulation -------------------------------------------------
```

The function MarkovChain simulates the networks from the ERGM with edge,
mutual and nodematch statistics

@param net an object of class `matrix`. Adjacency matrix of the network.
@param theta1 a numeric value. The value of the edge parameter of the
ERGM. @param theta2 a numeric value. The value of the mutual parameter
of the ERGM. @param theta3 a numeric value. The value of the istar(2)
parameter of the ERGM. @param burnin an integer value. Number of steps
to reach the stationary distribution. @param thinning an integer value.
Number of steps between simulated networks. @param nNet an integer
value. Number of simulated networks to return as output.

@return a named list: - netSim: an `array` with the adjancency matrices
of the simulated networks. - statSim: a `matrix` with the value of the
statistic defining the ERGM.

@examples MarkovChain( matrix(c(0, 1, 0, 0, 0, 0, 1, 1, 0), nrow = 3,
ncol = 3), -log(0.5), log(0.4), log(.8) )

```{r}
MarkovChain <- function(
    net,
    theta1, theta2, theta3,
    burnin = 10000, thinning = 1000, nNet = 1000){
  
  # Burnin phase: repeating the steps of the chain "burnin" times  
  nvertices <- nrow(net)
  burninStep <- 1 # counter for the number of burnin steps
  
  # Perform the burnin steps
  #                --- MISSING---
  # We let the random walk run for a long time to make sure that it does not depned on the starting network:
  for (i in 1:burnin) {
    net <- MHstep(net, theta1, theta2, theta3)
  }
  
  # After the burnin phase we draw the networks
  # The simulated networks and statistics are stored in the objects
  # netSim and statSim
  netSim <- array(0, dim = c(nvertices, nvertices, nNet))
  statSim <- matrix(0, nNet, 3)
  thinningSteps <- 0 # counter for the number of thinning steps
  netCounter <- 1 # counter for the number of simulated network
  
  #                --- MISSING---
  while (netCounter <= nNet){
    # compute the new network:
    net <- MHstep(net, theta1, theta2, theta3) 
    
    # compute the new statistics: 
    edges <- sum(net)
    dyads <- sum(net * t(net) * upper.tri(net))
    indegree <- colSums(net)
    in2stars <- sum(choose(indegree, 2))
    
    # update:
    netSim[,,netCounter] <- net
    statSim[netCounter,] <- c(edges, dyads, in2stars)
    netCounter <- netCounter + 1
    
    # thinning: simulate thinning many networks and throw them away
    for (i in 1:thinning){
      net <- MHstep(net, theta1, theta2, theta3)
    }
  }
  
  # Return the simulated networks and the statistics
  return(list(netSim = netSim, statSim = statSim))
}
################################################### 
#################### part (2) #####################

observed_net <- matrix(c(0, 1, 0, 0, 0, 0, 1, 1, 0), nrow = 3, ncol = 3)

# statistics of the observed network:
edges <- sum(observed_net)
dyads <- sum(observed_net * t(observed_net) * upper.tri(observed_net))
indegree <- colSums(observed_net)
in2stars <- sum(choose(indegree, 2))
observed_stats <- c(edges, dyads, in2stars)

# suggested parameters: 
theta <- c(-2.76, 0.68, 0.05 )

# simulate the MarkovChain:
list_simulation <- MarkovChain(observed_net, theta[1], theta[2], theta[3])

# analyse the moment equation to see if the parameters are reasonable
sample_equivalent <- colMeans(list_simulation$statSim)  # this is the formulate on slide 22
moment_eq <- sample_equivalent - observed_stats         # this should be closed to 0 if the parameters are good
print(paste("Values of the moment equation for the given parameters:", paste(moment_eq, collapse = ", ")))
```

Conclusion: The values of the moments equation should be close to 0 if
the parameters are a good choice. This is not the case. Hence, we should
search for better parameters.

```{r}
################################################### 
#################### part (3) #####################
```

We update the parameters in virtue of the Robbins-Monro algorithm on
slide 23:

```{r}
num_it = 5  # here I tried different values in {1,...,10}
for (n in 1:num_it){
  a = 1/n^2   # this needs to be a sequence converging to 0
  theta <- theta - a*moment_eq # update via formula on slide 23 (with D = I)
  # simulate the MarkovChain and compute the value of the moment equation
  list_simulation <- MarkovChain(observed_net, theta[1], theta[2], theta[3])
  sample_equivalent <- colMeans(list_simulation$statSim)  
  moment_eq <- sample_equivalent - observed_stats       
}

# check if the new parameters are better:
list_simulation <- MarkovChain(observed_net, theta[1], theta[2], theta[3])
sample_equivalent <- colMeans(list_simulation$statSim)  # this is the formulate on slide 22
moment_eq <- sample_equivalent - observed_stats         # this should be closed to 0 if the parameters are good
print(paste("Values of the moment equation for the updated parameters:", paste(moment_eq, collapse = ", ")))
print(paste("Updated theta:", paste(theta, collapse = ", ")))
```

Conclusion: As we can see, the updated parameters yield a smaller value
of the moment equation. Since we want the value of the moment equation
to be close to 0, the new parameters are better than the previous ones.

## Task 3

```{r}
# install.packages(c("robustbase"), lib="~/R/library", repos = "https://cloud.r-project.org/")
# install.packages(c("sna","network","ergm","igraph"), lib="~/R/library", repos = "https://cloud.r-project.org/")

library(sna)
library(network)
library(ergm) 

nodes <- read.table("Dataset/Krackhardt-High-Tech_nodes.txt", header = TRUE)
edges <- read.table("Dataset/Krackhardt-High-Tech_multiplex.edges", header = TRUE)
layers <- read.table("Dataset/Krackhardt-High-Tech_layers.txt", header = TRUE) 

head(nodes)
head(edges)

layer_to_analyze <- 2
layer_edges <- edges[edges$layerID == layer_to_analyze,]

net <- network(layer_edges[, c("nodeID1", "nodeID2")], directed = TRUE)

net%v%"age" <- nodes$nodeAge
net%v%"tenure" <- nodes$nodeTenure
net%v%"level" <- nodes$nodeLevel
net%v%"department" <- nodes$nodeDepartment
cat("Network for Layer:", layer_to_analyze, "\n")
print(net)

dept_colors <- c("0" = "red",      # CEO
                 "1" = "blue",     
                 "2" = "green",   
                 "3" = "purple",   
                 "4" = "orange")  

vertex_colors <- dept_colors[as.character(get.vertex.attribute(net, "department"))]

plot(net, vertex.col = vertex_colors, displaylabels = TRUE, 
     main = paste("Network Visualization -", layers$layerLabel[layers$layerID == layer_to_analyze]))

set.seed(1984) #literally 1984

```

### Task 3.1

```{r}
model0 <- ergm(net ~ edges + nodematch("department"))

summary(model0)

theta <- coef(model0)

p <- exp(sum(theta)) / (1 + exp(sum(theta)))
```

The conditional probability of observing a tie between i and j assuming
they work in the same department is:

```{r}
p
```

### Task 3.2

```{r}
model1 <- ergm(net ~ edges 
               + nodematch("department")  
               + mutual                                 # (i)
               + gwesp(decay = 0.3, fixed = TRUE)       # (ii)
               + gwidegree(decay = 0.3, fixed = TRUE))  # (iii)
```

### Task 3.3

```{r}
mcmc.diagnostics(model1)
```

In the MCDC diagnostic we can look at the traceplots for the different
terms. They describe the deviation of the statistic value in each
sampled network from the observed value, on the right we have the
distribution of the sample statistic deviations. As they describe in
[*here*](https://cran.r-project.org/web/packages/ergm/vignettes/ergm.pdf)
one should look for a "fuzzy caterpillar" shape in the traceplot which
we have for all terms except for the last one which indicates good
mixing and stability. In the last term there are some strong outliers
into the negative but there is no trend away from the observed value.
Also the autocorrelation falls off quickly for each term so we conclude
that the model is stable and converged nicely.

### Task 3.4

```{r}
model1gof <- gof(model1)
model1gof

par(mfrow=c(2,2),mar=c(5, 4, 4, 2))
plot(model1gof)
```

The plot shows that there is no significant difference between the
average simulation and the observed value as the black lines (observed
values) almost never cross the gray confidence interval. There are two
exceptions for odegree at value 18 and wide-wise shared partners at
value 4. Overall criteria, the observed network falls close to the
center of the gray regions of the simulation distribution thus the model
is a good fit.

```{r}
summary(model1)
```

$\alpha = 0.01$

Edges: The parameter is significant and negative thus the network is
sparse.

Department homophily: The parameter is not significant thus the data
doesn't support that ties are more likely within a department.

Mutal: The parameter is significant and positive thus it indicates
evidence for reciprocity.

Transitivity: The parameter is significant and positive thus it
indicates evidence for transitivity.

Popularity: The parameter is not significant thus the data doesn't show
that ties are more likely when the receiver has a higher in-degree.

## Task 4

### Task 4.1

```{r}
model2 <- ergm(net ~ edges 
               + nodematch("department")  
               + nodeocov("tenure")
               + absdiff("age")) 

summary(model2)


```

```{r}
# TODO MR-QAP model
```

TODO: I think this is going to happen when we compare them: When
structural effects are not included in the ERGM, its estimates closely
resemble those from MR-QAP.

```{r}
model3 <- ergm(net ~ edges 
               + nodematch("department")  
               + nodeocov("tenure")
               + absdiff("age")
               + mutual 
               + gwesp(decay = 0.3, fixed = TRUE) 
               + gwidegree(decay = 0.3, fixed = TRUE)) 

summary(model3)
```

### Task 4.2

Hypothesis that can be tested with ERGMs but not with MR-QAPs: $H_0$:
"There are triangles in the advice network." In order to test this
hypothesis with an ERGM we need to include the following statistics: $$
z_m(x) = \sum_{j,k,l} x^a_{jk} x^a_{jl} x^a_{lk},
$$ where $(x_{ij})_{ij}$ denotes the adjacency matrix of the advice
network.

TODO: State your hypothesis and provide the mathematical formula and the
**graphical representation** of the effect that you need to include in
the ERGM to test the hypothesis
